{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment, the goal is to reduce the model size of a deep neural network. This action will result on lighter and faster model. We will rely on the PyTorch functionalities for quantizing neural networks. The autonomous driving models from the previous assignments will be used for that purpose. \n",
    "\n",
    "In our context the process of quantization will convert the floating point parameters (32-bit, single precision) to integer parameters.\n",
    "\n",
    "Note that all scripts should be self-contained and executed on *any* machine that has required libraries installed.\n",
    "\n",
    "**Important**: There is a helpful tutorial on quantization at [https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will make use of the feed-forward neural network from the autonomous driving assignment. The goal is to analyze is in terms of of inference time, [FLOPS](https://pypi.org/project/thop/) (floating operations), [model size](https://discuss.pytorch.org/t/finding-model-size/130275) (in MB) and accuracy (classification problem). This task does not require training. A pre-trained model from the previous assignments can be employed.\n",
    "\n",
    "*Task Output*: The feed-forward model from the autonomous driving assignment should be used in order to compute the execution time, FLOPS, model size and accuracy. For that reason, one function for each metric should be created. The same functions will be later used for evaluating the quantized model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetclass with augmentations for CNN model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CarRacingDataset(Dataset):\n",
    "    def __init__(self, npz_file, augmentations=False, horizontal_flip=False,\n",
    "                 random_rotation=False, vertical_flip=False, street_color_change=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npz_file (str): Path to the .npz file containing 'frames' and 'actions'.\n",
    "            augmentations (bool): Whether to apply augmentations.\n",
    "        \"\"\"\n",
    "        data = np.load(npz_file)\n",
    "        self.frames = data['frames']\n",
    "        self.actions = data['actions']\n",
    "        self.augmentations = augmentations\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.random_rotation = random_rotation\n",
    "        self.vertical_flip = vertical_flip\n",
    "        self.street_color_change = street_color_change\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        action = self.actions[idx].copy()  # Prevent in-place edit\n",
    "\n",
    "        image = self.to_pil(frame)\n",
    "\n",
    "        # Horizontal flip: must flip steering direction\n",
    "        if random.random() < 0.2 and self.horizontal_flip and self.augmentations:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            action[0] = -action[0]  # Flip steering\n",
    "\n",
    "        # Random rotation (small angles)\n",
    "        if random.random() < 0.2 and self.random_rotation and self.augmentations:\n",
    "            angle = random.uniform(-20, 20)\n",
    "            image = image.rotate(angle)\n",
    "\n",
    "        # Vertical flip (no action change)\n",
    "        if random.random() < 0.2 and self.vertical_flip and self.augmentations:\n",
    "            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "        # Street color change\n",
    "        if random.random() < 0.2 and self.street_color_change and self.augmentations:\n",
    "            gray_min = np.array([100, 100, 100])\n",
    "            gray_max = np.array([150, 150, 150])\n",
    "            brown_min = np.array([90, 60, 30])\n",
    "            brown_max = np.array([150, 100, 60])\n",
    "            img_np = np.array(image)\n",
    "            mask = np.all((img_np >= gray_min) & (img_np <= gray_max), axis=2)\n",
    "            brown_color = np.array([random.randint(brown_min[i], brown_max[i]) for i in range(3)], dtype=np.uint8)\n",
    "            img_np[mask] = brown_color\n",
    "            image = Image.fromarray(img_np)\n",
    "\n",
    "        image = self.to_tensor(image)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "        return image, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN network architecture used as before\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define CNN model for 96x96 images and 3 value o/p\n",
    "class CarCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CarCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),      # Output: [steer, gas, brake]\n",
    "            nn.Tanh()               # Ensure outputs in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "from thop import profile\n",
    "\n",
    "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            inputs = inputs.to(device)\n",
    "            start = time.time()\n",
    "            _ = model(inputs)\n",
    "            end = time.time()\n",
    "            total_time += (end - start)\n",
    "    avg_time = total_time / num_batches\n",
    "    return avg_time\n",
    "\n",
    "# Function 2: Compute FLOPs and parameters\n",
    "def compute_flops(model, input_size, device):\n",
    "    dummy_input = torch.randn(*input_size).to(device)\n",
    "    flops, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "    return flops, params\n",
    "\n",
    "# Function 3: Compute model size in MB\n",
    "def compute_model_size(model_path):\n",
    "    size_bytes = os.path.getsize(model_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "# Function 4: Compute regression MSE instead of classification accuracy\n",
    "def compute_regression_mse(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_mse = 0\n",
    "    count = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_mse += loss.item() * inputs.size(0)\n",
    "            count += inputs.size(0)\n",
    "    \n",
    "    return total_mse / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgudi\\AppData\\Local\\Temp\\ipykernel_29968\\1320064756.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per batch: 0.0170 seconds\n",
      "FLOPs: 66.87 Mega FLOPs\n",
      "Model size: 9.99 MB\n",
      "Mean Squared Error for regression: 0.0312\n"
     ]
    }
   ],
   "source": [
    "#calculate the metrics for the CNN model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#load the model\n",
    "model_path = \"car_cnn_final_augmented.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"CPU\")\n",
    "model = CarCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "npz_file_path = \"continuous_15k_dataset.npz\"\n",
    "# create the Dataset\n",
    "dataset = CarRacingDataset(\n",
    "    npz_file=npz_file_path,\n",
    "    augmentations=True, #making augmentations to false here renders the other parameters useless\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    street_color_change=True\n",
    ")\n",
    "\n",
    "testLoader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "# Measure inference time\n",
    "inference_time = measure_inference_time(model, testLoader, device)\n",
    "print(f\"Inference time per batch: {inference_time:.4f} seconds\")\n",
    "# Compute FLOPs\n",
    "input_size = (1, 3, 96, 96)  # Batch size 1, 3 channels, 96x96 image\n",
    "flops, _ = compute_flops(model, input_size, device)\n",
    "print(f\"FLOPs: {flops / 1e6:.2f} Mega FLOPs\")  # Convert to GFLOPs\n",
    "# Compute model size\n",
    "model_size = compute_model_size(model_path)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "# Compute accuracy on test set\n",
    "mse = compute_regression_mse(model, testLoader, device)\n",
    "print(f\"Mean Squared Error for regression: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetclass with augmentations for CNN + RNN model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CarRacingEpisodeDataset(Dataset):\n",
    "    def __init__(self, npz_path, sequence_length=10,\n",
    "                 augmentations=True, horizontal_flip=True,\n",
    "                 random_rotation=True, vertical_flip=False,\n",
    "                 street_color_change=False):\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        raw_episodes = data[\"episodes\"]\n",
    "\n",
    "        # Unpack all episodes into frame-action pairs with episode index\n",
    "        self.sequences = []  # Each entry: (episode_idx, start_idx)\n",
    "        self.episodes = []\n",
    "\n",
    "        for episode_idx, (frames, actions) in enumerate(raw_episodes):\n",
    "            frames = np.array(frames)\n",
    "            actions = np.array(actions)\n",
    "            self.episodes.append((frames, actions))\n",
    "            if len(frames) >= sequence_length:\n",
    "                for i in range(len(frames) - sequence_length + 1):\n",
    "                    self.sequences.append((episode_idx, i))\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.augmentations = augmentations\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.random_rotation = random_rotation\n",
    "        self.vertical_flip = vertical_flip\n",
    "        self.street_color_change = street_color_change\n",
    "\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def _apply_horizontal_flip(self, image, action):\n",
    "        if random.random() < 0.2 and self.horizontal_flip and self.augmentations:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            action[0] = -action[0]  # flip steering\n",
    "        return image, action\n",
    "\n",
    "    def _apply_random_rotation(self, image):\n",
    "        if random.random() < 0.2 and self.random_rotation and self.augmentations:\n",
    "            angle = random.uniform(-20, 20)\n",
    "            return image.rotate(angle)\n",
    "        return image\n",
    "\n",
    "    def _apply_vertical_flip(self, image):\n",
    "        if random.random() < 0.2 and self.vertical_flip and self.augmentations:\n",
    "            return image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return image\n",
    "\n",
    "    def _apply_street_color_change(self, image):\n",
    "        if random.random() < 0.2 and self.street_color_change and self.augmentations:\n",
    "            gray_min = np.array([100, 100, 100])\n",
    "            gray_max = np.array([150, 150, 150])\n",
    "            brown_min = np.array([90, 60, 30])\n",
    "            brown_max = np.array([150, 100, 60])\n",
    "            img_np = np.array(image)\n",
    "            mask = np.all((img_np >= gray_min) & (img_np <= gray_max), axis=2)\n",
    "            brown_color = np.array([random.randint(brown_min[i], brown_max[i]) for i in range(3)], dtype=np.uint8)\n",
    "            img_np[mask] = brown_color\n",
    "            return Image.fromarray(img_np)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        episode_idx, start_idx = self.sequences[idx]\n",
    "        frames, actions = self.episodes[episode_idx]\n",
    "        frames_seq = []\n",
    "        actions_seq = []\n",
    "\n",
    "        for i in range(self.sequence_length):\n",
    "            frame = frames[start_idx + i]\n",
    "            action = actions[start_idx + i].copy()\n",
    "\n",
    "            image = self.to_pil(frame)\n",
    "            image, action = self._apply_horizontal_flip(image, action)\n",
    "            image = self._apply_random_rotation(image)\n",
    "            image = self._apply_vertical_flip(image)\n",
    "            image = self._apply_street_color_change(image)\n",
    "\n",
    "            image = self.to_tensor(image)\n",
    "            frames_seq.append(image)\n",
    "            actions_seq.append(torch.tensor(action, dtype=torch.float32))\n",
    "\n",
    "        return torch.stack(frames_seq), torch.stack(actions_seq)  # [T, C, H, W], [T, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RNN network architecture used as before\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "class CarCNN_RNN(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=1):\n",
    "        super(CarCNN_RNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2),  # 96x96 -> 48x48\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5, stride=2, padding=2),  # 48x48 -> 24x24\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2),  # 24x24 -> 12x12\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.feature_size = 128 * 12 * 12\n",
    "        self.rnn = nn.GRU(self.feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Output: [steer, gas, brake]\n",
    "        # It's often better to have separate heads for different action types\n",
    "        self.fc_steer = nn.Linear(hidden_size, 1)\n",
    "        self.fc_gas = nn.Linear(hidden_size, 1)\n",
    "        self.fc_brake = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(T):\n",
    "            out = self.cnn(x[:, t])  # [B, 128, 12, 12]\n",
    "            out = out.reshape(B, -1) # [B, 18432]\n",
    "            cnn_features.append(out)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)  # [B, T, feature_size]\n",
    "        rnn_out, _ = self.rnn(cnn_features)              # [B, T, hidden_size]\n",
    "\n",
    "        # Apply activations per action type\n",
    "        steer = torch.tanh(self.fc_steer(rnn_out))  # [-1, 1]\n",
    "        gas = torch.sigmoid(self.fc_gas(rnn_out))    # [0, 1]\n",
    "        brake = torch.sigmoid(self.fc_brake(rnn_out)) # [0, 1]\n",
    "\n",
    "        # Concatenate outputs\n",
    "        out = torch.cat((steer, gas, brake), dim=-1) # [B, T, 3]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgudi\\AppData\\Local\\Temp\\ipykernel_29968\\255550887.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per batch: 0.0089 seconds\n",
      "FLOPs: 716.41 Mega FLOPs\n",
      "Model size: 28.18 MB\n",
      "Mean Squared Error for regression: 0.2729\n"
     ]
    }
   ],
   "source": [
    "#calculate the metrics for the RNN model\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#load the model\n",
    "model_path = \"unfiltered_IP_cnn_rnn_final_continuous.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"CPU\")\n",
    "model = CarCNN_RNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "npz_file_path = \"Unfiltered_IP_CNN_RNN_continuous_15k_dataset.npz\"\n",
    "# create the Dataset\n",
    "dataset = CarRacingEpisodeDataset(npz_path=npz_file_path)\n",
    "\n",
    "testLoader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "# Measure inference time\n",
    "inference_time = measure_inference_time(model, testLoader, device)\n",
    "print(f\"Inference time per batch: {inference_time:.4f} seconds\")\n",
    "# Compute FLOPs\n",
    "input_size = (1, 10, 3, 96, 96)  # Batch size 1, sequence length 10, 3 channels, 96x96 image\n",
    "flops, _ = compute_flops(model, input_size, device)\n",
    "print(f\"FLOPs: {flops / 1e6:.2f} Mega FLOPs\")  # Convert to GFLOPs\n",
    "# Compute model size\n",
    "model_size = compute_model_size(model_path)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "# Compute accuracy on test set\n",
    "mse = compute_regression_mse(model, testLoader, device)\n",
    "print(f\"Mean Squared Error for regression: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Static Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. The quantization is static and thus it does not include any training process. \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization).\n",
    "\n",
    "*Task Output*: The weights of the model are float variables. They should be converted to int. Then the execution time, FLOPS, model size and accuracy should be computed and compared to the original model.\n",
    "\n",
    "*Important*: Quantization is possible on the eager mode of PyTorch. This requires to install another version of PyTorch.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define quantised CNN network architecture with quantisation stubs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define quantised CNN model for 96x96 images and 3 value o/p\n",
    "class QuantizedCarCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedCarCNN, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.net(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pgudi\\.conda\\envs\\MLSP_Lab\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Quantized] Inference time per batch: 0.0140 seconds\n",
      "[Quantized] FLOPs: 0.00 Mega FLOPs\n",
      "[Quantized] Model size: 2.52 MB\n",
      "[Quantized] Mean Squared Error for regression: 0.0607\n"
     ]
    }
   ],
   "source": [
    "#calculate the metrics for the quantised CNN model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#load the model\n",
    "model_path = \"car_cnn_final_augmented.pth\"\n",
    "device_cpu = torch.device('cpu')\n",
    "\n",
    "model_fp32  = QuantizedCarCNN().to(device_cpu)\n",
    "model_fp32 .load_state_dict(torch.load(model_path, map_location=device_cpu, weights_only=True))\n",
    "#move the model to cpu for quantization\n",
    "model_fp32 .eval()\n",
    "model_fp32 .to('cpu')\n",
    "\n",
    "# Set quantization config\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare model for static quantization\n",
    "torch.quantization.prepare(model_fp32, inplace=True)\n",
    "\n",
    "npz_file_path = \"continuous_15k_dataset.npz\"\n",
    "# create the Dataset\n",
    "dataset = CarRacingDataset(\n",
    "    npz_file=npz_file_path,\n",
    "    augmentations=True, #making augmentations to false here renders the other parameters useless\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    street_color_change=True\n",
    ")\n",
    "\n",
    "testLoader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Calibration with representative data\n",
    "with torch.no_grad():\n",
    "    for images, _ in testLoader:\n",
    "        model_fp32(images)  # no labels needed\n",
    "        break  # Only a small batch is needed for calibration\n",
    "\n",
    "# Convert to quantized model\n",
    "model_quantized = torch.quantization.convert(model_fp32, inplace=False)\n",
    "\n",
    "# Move quantized model to CPU\n",
    "model_quantized.to(device_cpu)\n",
    "\n",
    "# Measure inference time\n",
    "inference_time = measure_inference_time(model_quantized, testLoader, device_cpu)\n",
    "print(f\"[Quantized] Inference time per batch: {inference_time:.4f} seconds\")\n",
    "# Compute FLOPs\n",
    "input_size = (1, 3, 96, 96)  # Batch size 1, 3 channels, 96x96 image\n",
    "flops, _ = compute_flops(model_quantized, input_size, device_cpu)\n",
    "print(f\"[Quantized] FLOPs: {flops / 1e6:.2f} Mega FLOPs\")  # Convert to GFLOPs\n",
    "# Compute model size\n",
    "quantized_model_path = \"car_cnn_quantized.pth\"\n",
    "torch.save(model_quantized.state_dict(), quantized_model_path)\n",
    "model_size = compute_model_size(quantized_model_path)\n",
    "print(f\"[Quantized] Model size: {model_size:.2f} MB\")\n",
    "# Compute accuracy on test set\n",
    "mse = compute_regression_mse(model_quantized, testLoader, device_cpu)\n",
    "print(f\"[Quantized] Mean Squared Error for regression: {mse:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization-Aware Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized and trained at the same time. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. This type of training is called Quantization-aware training (QAT). \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization)\n",
    "\n",
    "*Task Output*: The already quantized model from the previous will be use to conduct the training process. The model should be trained until convergence. Then, then the execution time, FLOPS, model size and accuracy should be computed and compared to the static quantization and the original model.\n",
    "\n",
    "*Important*: Quantization is possible on the eager mode of PyTorch. This requires to install another version of PyTorch.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgudi\\AppData\\Local\\Temp\\ipykernel_29968\\386031014.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quantised_weights = torch.load(\"car_cnn_final_augmented.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedCarCNN(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(\n",
       "      3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(\n",
       "      32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(\n",
       "      64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(\n",
       "      in_features=18432, out_features=128, bias=True\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (8): ReLU()\n",
       "    (9): Linear(\n",
       "      in_features=128, out_features=3, bias=True\n",
       "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      )\n",
       "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (10): Tanh(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0078]), zero_point=tensor([128], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # QAT requires CPU execution\n",
    "\n",
    "QATmodel = QuantizedCarCNN().to(device)\n",
    "\n",
    "# Load original FP32 weights\n",
    "quantised_weights = torch.load(\"car_cnn_final_augmented.pth\", map_location=device)\n",
    "QATmodel.load_state_dict(quantised_weights)\n",
    "\n",
    "# Set QAT config\n",
    "QATmodel.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare_qat(QATmodel, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.0171\n",
      "[Epoch 2] Loss: 0.0163\n",
      "[Epoch 3] Loss: 0.0158\n",
      "[Epoch 4] Loss: 0.0154\n",
      "[Epoch 5] Loss: 0.0151\n",
      "[Epoch 6] Loss: 0.0149\n",
      "[Epoch 7] Loss: 0.0144\n",
      "[Epoch 8] Loss: 0.0143\n",
      "[Epoch 9] Loss: 0.0139\n",
      "[Epoch 10] Loss: 0.0136\n"
     ]
    }
   ],
   "source": [
    "#training of the QAT model\n",
    "\n",
    "# Dataset\n",
    "dataset = CarRacingDataset(\n",
    "    npz_file=\"continuous_15k_dataset.npz\",\n",
    "    augmentations=False\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = optim.Adam(QATmodel.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# QAT Training Loop\n",
    "QATmodel.train()\n",
    "num_epochs = 10  # Increase until convergence\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = QATmodel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] Inference time per batch: 0.0102 seconds\n",
      "[QAT] FLOPs: 0.00 Mega FLOPs\n",
      "[QAT] Model size: 2.52 MB\n",
      "[QAT] Mean Squared Error: 0.0134\n"
     ]
    }
   ],
   "source": [
    "# calculate the metrics for the QAT model\n",
    "\n",
    "#convert model to quantized version\n",
    "QATmodel.eval()\n",
    "QATmodel_quantized = torch.quantization.convert(QATmodel.eval(), inplace=False)\n",
    "\n",
    "# Time\n",
    "inference_time = measure_inference_time(QATmodel_quantized, train_loader, device)\n",
    "print(f\"[QAT] Inference time per batch: {inference_time:.4f} seconds\")\n",
    "\n",
    "# FLOPs\n",
    "input_size = (1, 3, 96, 96)\n",
    "flops, _ = compute_flops(QATmodel_quantized, input_size, device)\n",
    "print(f\"[QAT] FLOPs: {flops / 1e6:.2f} Mega FLOPs\")\n",
    "\n",
    "# Size\n",
    "qat_model_path = \"car_cnn_qat.pth\"\n",
    "torch.save(QATmodel_quantized.state_dict(), qat_model_path)\n",
    "model_size = compute_model_size(qat_model_path)\n",
    "print(f\"[QAT] Model size: {model_size:.2f} MB\")\n",
    "\n",
    "# Accuracy (MSE)\n",
    "mse = compute_regression_mse(QATmodel_quantized, train_loader, device)\n",
    "print(f\"[QAT] Mean Squared Error: {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLSP_Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
